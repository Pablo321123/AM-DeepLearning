{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nota√ß√£o matem√°tica utilizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos uma nota√ß√£o matem√°tica similar a da pr√°tica de regress√£o log√≠stica, com algumas modifica√ß√µes. Elementos modificados est√£o em azul, novos est√£o em verdes e, n√£o modificados, em preto.\n",
    "\n",
    "- $m$: quantidade de instancias\n",
    "- $f$: quantidade de atributos\n",
    "- $n^{[l]}$: quantidade de unidades da camada $l$ \n",
    "- $X$: Matriz de inst√¢ncias representadas pelo seus atributos a ordem da matriz √© $m \\times f$\n",
    "- $\\pmb{y}$: Vetor de tamanho $m$ representando a classe real de cada inst√¢ncia\n",
    "- $\\pmb{\\hat{y}}$: Vetor de predi√ß√µes que, para cada instancia, possui o valor predito para ela. Caso seja uma classifica√ß√£o bin√°ria, este valor ser√° 0 ou 1\n",
    "\n",
    "- $Z^{[l]}$: Matriz, para cada instancia, o valor $z^{[l]}_{i,j}$ representando o resultado da fun√ß√£o `z` para instancia $i$ e unidade $j$. Essa matriz √© de ordem $m \\times n^{[l]}$\n",
    "\n",
    "- $A^{[l]}$ Matriz de ativa√ß√µes da camada $l$ em que, $a^{[l]}_{i,j}$ √© um elemento dessa matriz que representa a ativa√ß√£o da instancia $i$ na camada $l$ na unidade $j$. Dessa forma essa √© uma matriz de ordem $m \\times n^{[l]}$.\n",
    "\n",
    "- $W^{[l]}$ Matriz de pesos da camada $l$ em que cada elemento $w^{[l]}_{i,j}$ representa pondera√ß√£o que a unidade $i$ da camada $l$ faz na unidade $j$ da matriz de ativa√ß√£o $A^{[l-1]}$. Caso seja primeira camada, a pondera√ß√£o feita no atributo $j$ da matriz $X$. Dessa forma, essa matriz √© de ordem $n^{[l]} \\times n^{[l-1]}$ exceto na primeira camada que √© de ordem $n^{[l]} \\times f$\n",
    "\n",
    "- $Z^{d[l]}$: Derivada $\\frac{\\partial J}{\\partial z^{[l]}_{i,j}}$. Possui a mesma ordem que $Z$\n",
    "\n",
    "- $W^{d[l]}$: Derivada $\\frac{\\partial J}{\\partial w^{[l]}_{i,j}}$. Possui a mesma ordem que $W$\n",
    "\n",
    "Como em nossa implementa√ß√£o temos tamb√©m representado os elementos por unidade, tamb√©m usaremos a representa√ß√£o por unidade $j$ em uma determinada camada $l$, apresetnada a seguir. Note que s√£o os mesmos elementos da pr√°tica de regress√£o, por√©m, nesta pr√°tica, temos que definir a qual unidade $j$ e camada $l$ estamos nos referindo.\n",
    "\n",
    "- $\\pmb{z}^{[l]}_j$: $j$-√©sima coluna da matriz $Z^{[l]}$ representado o resultado da fun√ß√£o z, para cada instancia $m$ da unidade $j$ e camada $l$. Assim, este vetor possui o tamanho $m$.\n",
    "- $\\pmb{a}^{[l]}_j$: $j$-√©sima coluna da matriz $A^{[l]}$ representando o vetor de ativa√ß√µes da unidade $j$ de tamanho $m$ calculada por meio do vetor $z^[l]_j$\n",
    "- $\\pmb{w}^{[l]}_{j}$: j-√©sima **linha** da matriz $W^{[l]}$ que representa vetor de pesos de uma unidade (neur√¥nio) $j$ para ponderar os pesos da camada anterior ou, caso seja a primeira camada, pondera os atributos. Assim, esse vetor possui o tamanho $f$ (primeira camada) e tamanho  $n^{[l-1]}$ (demais camadas).\n",
    "\n",
    "\n",
    "- $b^{[l]}_j$: Valor de vi√©s da unidade $j$ e camada $l$ (do ingl√™s, bias term)\n",
    "<!--- <span class=\"blue\" style=\"color:blue\">$w_j$: $j$-√©simo valor do vetor $\\pmb{w}$ que pondera $j$-√©sima coluna da matriz $A^{[l]}$. Essa coluna representa a $j$-√©sima  unidade da camada anterior ou, quando for a primeira camada, o $j$-√©simo atributo da matriz $X$</span>.-->\n",
    "\n",
    "Al√©m das representa√ß√µes das derivadas por unidade $j$  em uma determinada camada $l$:\n",
    "\n",
    "- $\\pmb{z}^{d[l]}_j$: Vetor de derivada $\\frac{\\partial J}{\\partial z_j}$ para cada inst√¢ncia $i$ do modelo de uma determinada unidade $j$ e camada $l$ em uma determinada camada $l$. Possui o mesmo tamanho que  $\\pmb{z}$\n",
    "- $\\pmb{w}^{d[l]}_j$ Vetor de derivada $\\frac{\\partial J}{\\partial w_j}$ para cada unidade $j$ do modelo, possui o mesmo tamanho de $\\pmb{w}^{[l]}_{j}$\n",
    "- $b^{d[l]}_j$: Derivada $\\frac{\\partial J}{\\partial b^{[l]}_j}$ \n",
    "\n",
    "Perceba que esses vetores representam a $j$-√©sima coluna de suas respectivas matrizes, para cada unidade $j$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementa√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, apresentaremos as modifica√ß√µes que devem ser feitas por classe. Primeiramente, execute as importan√ß√µes que usaremos na pr√°tica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from rede_neural_profunda import *\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe FuncaoAtivacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 1 - Fun√ß√µes de ativa√ß√µes novas e altera√ß√£o na fun√ß√£o sigmoid**: Voc√™ agora poder√° ver 4 objetos da classe `FuncaoAtivacao` instanciados: sigmoid, relu, leaky_relu e tanh. Voc√™ dever√° modificar alterar as suas fun√ß√µes lambda para retornar o resultado correto, dependendo da fun√ß√£o de ativa√ß√£o.\n",
    "\n",
    "O valor do vetor de derivadas $\\pmb{z^{d[l]}_j}$ √© diferente caso estejamos na √∫ltima camada. Por isso, na classe `FuncaoAtivacao` agora temos um atributo (opcional) `dz_ultima_camada` que √© a fun√ß√£o da derivada quando  calculamos o vetor $\\pmb{z^{d[l]}_j}$ na √∫ltima camada com uma determinada fun√ß√£o de ativa√ß√£o. A fun√ß√£o de ativa√ß√£o `sigmoid` ser√° a √∫nica que usaremos como √∫ltima camada, assim, altere o objeto `sigmoid`com a fun√ß√£o  `dz_ultima_camada` correspondente. \n",
    "\n",
    "O atributo `dz_ultima_camada` do objeto sigmoid ter√° a fun√ß√£o que usamos como `dz_funcao`  na pr√°tica passada. Agora, o parametro `dz_funcao` ser√° diferente: temos que usar o par√¢metro `arr_dz_w_prox` representando a $j$-√©sima coluna do produto $Z^{d[l+1]} \\times W^{[l+1]}$. Crie tamb√©m as fun√ß√µes de ativa√ß√£o relu, leaky_relu e tangente hiperb√≥lica (vari√°veis `relu`, `leaky_relu` e `tanh`, respectivamente) usando apenas os par√¢metros do construtor `funcao` e `dz_funcao`. Lembrando que n√£o usaremos as fun√ß√µes de ativa√ß√£o `relu`, `leaky_relu` e `tanh` como √∫ltima camada (camada de sa√≠da). Para implementar essas express√µes lambda, voc√™ poder√° usar vetoriza√ß√£o. Lembre-se do seguinte c√≥digo da pr√°tica de regress√£o log√≠stica: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True  True False]\n",
      "[0 0 3 3 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "meu_querido_vetor = np.array([3,2,8,9,2])\n",
    "#a linha abaixo retorna true ou false, dependendo do valor\n",
    "print(meu_querido_vetor>4)\n",
    "#Se multiplicamos um n√∫mero por um vetor numpy de true e false \n",
    "#. √© o mesmo de multiplicarmos o n√∫mero por 1 ou 0, respectivamente\n",
    "print(3*(meu_querido_vetor>4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestFuncaoAtivacao.test_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestFuncaoAtivacao.test_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestFuncaoAtivacao.test_leaky_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestFuncaoAtivacao.test_tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altera√ß√£o da classe `RegressaoLogistica` para `Unidade`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classe Unidade:** A classe RegressaoLogistica da pr√°tica anterior foi renomeada para Unidade.  Essa classe j√° est√° pronta, agora, ela representar√° uma unidade (neur√¥nio). Alguns atributos  foram removidos (pois agora n√£o criamos o modelo apenas com uma unidade) e um √∫nico atributo foi modificado: tinhamos antes o `mat_x`, representando a matriz $X$, ao inv√©s dele, temos que utilizar a matriz $A^{[l-1]}$, ou seja, a matriz de ativa√ß√µes da camada anterior. Essa matriz, representada pelo atributo `mat_a_ant`, √© usada no lugar de `mat_x` para calcular o `forward_propagation` e o `backward_propagation`. Reveja a implementa√ß√£o e relembre os atributos dela:\n",
    "\n",
    "- `b`: Valor $b$ de vi√©s da regress√£o log√≠stica \n",
    "- `func_ativacao`: fun√ß√£o de ativa√ß√£o a ser usada. Esse atributo √© uma fun√ß√£o Python\n",
    "- `dz_func`: fun√ß√£o derivada a ser usada de acordo com a fun√ß√£o de ativa√ß√£o\n",
    "- `arr_w`: vetor de pesos $\\pmb{w^{[l]}_{j}}$\n",
    "- `arr_z`: vetor de resultados $\\pmb{z^{[l]}_{j}}$ \n",
    "- `arr_a`: vetor de ativa√ß√µes $\\pmb{a^{[l]}_{j}}$\n",
    "- `mat_a_ant`: Matriz de ativa√ß√µes da camada anterior $A^{[l-1]}$ (ou matriz $X$, caso esta seja a primeira camada)\n",
    "- `gradiente`: Instancia da classe `Gradiente` que possui os atributos `arr_dz`, `arr_dw` e `db` representando, respectivamente, $\\pmb{z^{d[l]}_j}$, $\\pmb{w^{d[l]}_j}$ e $b^{d[l]}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementa√ß√£o da classe Camada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe camada possui os seguintes atributos:\n",
    "\n",
    "- `arr_unidades`: Vetor de unidades (neur√¥nios) desta camada. Representada por inst√¢ncias da classe `Unidade`. S√£o inicializados por meio dos parametros do construtor que definem a quantidade de unidades, fun√ß√£o de ativa√ß√£o e derivada (`qtd_unidades`, `func_ativacao` e `func_dz` respectivamente).\n",
    "- `ant_camada`: Inst√¢ncia da classe `Camada` referente √† camada anterior. Caso seja a primeira camada escondida, ent√£o `ant_camada == None`\n",
    "- `prox_camada`: Inst√¢ncia da classe `Camada` referente √† proxima camada. Caso seja a √∫ltima camada (camada de sa√≠da), ent√£o `prox_camada == None`\n",
    "- `qtd_un_camada_ant`: Quantidade de unidades da camada anterior, armazena o valor $n^{[l-1]}$ exceto na primeira camada que recebe o valor $f$ \n",
    "- `mat_w`: atributo calculado (`property`) representando a matriz de pesos $W^{[l]}$ desta camada\n",
    "- `mat_dz`: atributo calculado (`property`) representando a derivada $Z^{d[l]}$. \n",
    "- `mat_dz_w`: Atributo calculado que efetua o produto $Z^{d[l]} \\times W^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 2 - construtor da classe `Camada`**: Inicialize o vetor de `arr_unidades` com a quantidade de unidades correspondente (use os par√¢metros do construtor da classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestCamada.test_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 3 - m√©todo `forward_propagation` da classe `Camada`**: Nesse m√©todo, voc√™ dever√° criar a matriz de ativa√ß√£o $A^{[l]}$ da camada atual $l$ (representada por `mat_a`) por meio da matriz de ativa√ß√µes anterior $A^{[l-1]}$ (representada por `mat_a_ant`). Para isso, voc√™ dever√° chamar o m√©todo `forward_propagation` de cada unidade (classe Unidade), implementado por voc√™s na pr√°tica de Regress√£o Log√≠stica, que cria o vetor $\\pmb{a^{[l]}_j}$, representado por `arr_a` de cada unidade. Para criar a matriz `mat_a`, observe sempre a sua dimens√£o. Dica: veja abaixo uma forma de preencher valores em linhas/colunas de uma matriz numpy por meio de vetores. Voc√™ dever√° usar [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html) para inicializar a matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[0.60060552 0.96332702 0.85980895 0.07391275 0.72683987]\n",
      "[0.18856964 0.73532341 0.59876395 0.74630847 0.67716601 0.55231282\n",
      " 0.89027868 0.23294579 0.86009492 0.81632344]\n"
     ]
    }
   ],
   "source": [
    "#considere a matriz mat_h e o vetor arr_linha e arr_coluna:\n",
    "mat_h = np.zeros((10,5))\n",
    "arr_linha = np.random.rand(5)\n",
    "arr_coluna = np.random.rand(10)\n",
    "print(mat_h)\n",
    "print(arr_linha)\n",
    "print(arr_coluna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.60060552, 0.96332702, 0.85980895, 0.07391275, 0.72683987],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#para preenchermos a linha 3 com o vetor arr_linha:\n",
    "mat_h[3,:] = arr_linha\n",
    "mat_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.18856964, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.73532341, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.59876395, 0.        , 0.        ],\n",
       "       [0.60060552, 0.96332702, 0.74630847, 0.07391275, 0.72683987],\n",
       "       [0.        , 0.        , 0.67716601, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.55231282, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.89027868, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.23294579, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.86009492, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.81632344, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#para preenchermos a coluna 2 com o vetor arr_coluna: \n",
    "mat_h[:,2] = arr_coluna\n",
    "mat_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.008s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestCamada.test_forward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 4 - propriedades da classe `Camada`**: Voc√™ dever√° implementar as propriedades `mat_w` e `mat_dz` para criar as matrizes $W^{[l]}$ e $Z^{d[l]}$. Logo ap√≥s, voc·∫Ω dever√° implementar a propriedade `mat_dz_w` que calcula $Z^{d[l]} \\times W^{[l]}$. Fique atento nas dimens√µes das matrizes para usar o preenchimento de matrizes corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestCamada.test_mat_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestCamada.test_mat_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestCamada.test_mat_dz_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 5 - m√©todo backward_propagation da classe `Camada`:** Por meio da matriz `mat_dz_dw` da camada seguinte, ou seja, o resultado de $Z^{d[l+1]} \\times W^{[l+1]}$ e do vetor de classes $\\pmb{y}$, execute o backward propagation de todas as unidades. Para obter o `mat_dz_dw` da camada seguinte n√£o esque√ßa que as camadas possuem o atributo `prox_camada`. Caso essa seja a √∫ltima camada, ent√£o `mat_dz_dw == None`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "[-0.48577224  0.44051483  0.47762563 -0.30339597]\n",
      "[-0.67154448  0.44525741  0.66747117 -0.37669799]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestCamada.test_backward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementa√ß√£o da classe `RedeNeural`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe `RedeNeural` possui os seguintes atributos: \n",
    "\n",
    "- `arr_camadas`: Vetor que armazena a lista de instancias da classe `Camada` dessa Rede Neural. Esses objetos s√£o instanciados por meio do m√©todo `config_rede`\n",
    "- `arr_qtd_un_por_camada`: Vetor de inteiros que, o valor na posi√ß√£o `i` do vetor corresponde a quantidade de unidades na i-√©sima camada.\n",
    "- `arr_func_a_por_camada`: Fun√ß√£o de ativa√ß√£o por camada. Esse vetor armazena objetos da classe `FuncaoAtivacao` em que, cada posi√ß√£o `i` do vetor, corresponde a fun√ß√£o de ativa√ß√£o da i-√©sima camada\n",
    "- `num_iteracoes`: N√∫mero de itera√ß√µes (√©pocas) que a rede neural ir√° rodar\n",
    "- `arr_y`: Representa o vetor $\\pmb{y}$. Esses objetos s√£o instanciados por meio do m√©todo `config_rede`\n",
    "- `mat_x`: Representa a matriz de atributos por instancias $X$. Esses objetos s√£o instanciados por meio do m√©todo `config_rede`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 6 - m√©todo `config_rede`**: inst√¢ncia  armazena em arr_camada todas as camadas por meio da\n",
    "        quantidade de unidades por camada `arr_qtd_un_por_camada` e fun√ß√µes de ativa√ß√£o `arr_func_a_por_camada`. Para cada camada, voc√™ dever√° referenciar a camada anterior e a pr√≥xima por meio os atributos `ant_camada` e `prox_camada`, respectivamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C√≥digo Python √∫til para atividades 7 e 8:** Muitas vezes, precisamos percorrer vetores a partir de uma determinada posi√ß√£o ou de tr√°s para frente. Veja como isso pode ser feito usando a fun√ß√£o [range](https://docs.python.org/3/library/functions.html#func-range) e a fun√ß√£o [enumerate](https://docs.python.org/3/library/functions.html#enumerate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tras para frente: \n",
      "Pos arr_v[5]: 1\n",
      "Pos arr_v[4]: 3\n",
      "Pos arr_v[3]: 5\n",
      "Pos arr_v[2]: 4\n",
      "Pos arr_v[1]: 2\n",
      "Pos arr_v[0]: 4\n",
      "---\n",
      "A partir do 3o: \n",
      "Pos arr_v[3]: 5\n",
      "Pos arr_v[4]: 3\n",
      "Pos arr_v[5]: 1\n",
      "mesma efeito, com enumerate:\n",
      "Pos arr_v[3]: 5\n",
      "Pos arr_v[4]: 3\n",
      "Pos arr_v[5]: 1\n"
     ]
    }
   ],
   "source": [
    "arr_v = [4,2,4,5,3,1]\n",
    "tam_vet = len(arr_v)\n",
    "#percorrer elementos de tras para frente\n",
    "print(\"Tras para frente: \")\n",
    "for i in range(tam_vet-1,-1,-1):\n",
    "    print(f\"Pos arr_v[{i}]: {arr_v[i]}\")\n",
    "print(\"---\")\n",
    "print(\"A partir do 3o: \")\n",
    "#percorrer elementos a partir do indice 3\n",
    "for i in range(3,tam_vet):\n",
    "    print(f\"Pos arr_v[{i}]: {arr_v[i]}\")\n",
    "    \n",
    "#Com o enumerate, tamb√©m conseguimos percorrer a partir de uma posi√ß√£o\n",
    "#..Lembre que arr_v[3:] retorna o vetor a partir da terceira posi√ß√£o \n",
    "print(\"mesma efeito, com enumerate:\")\n",
    "for i,valor in enumerate(arr_v[3:],3):\n",
    "    print(f\"Pos arr_v[{i}]: {valor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-21 16:04:09,906] A new study created in memory with name: no-name-c71b6297-3c4d-4a00-a53b-bafe10de6712\n",
      "[I 2023-10-21 16:04:09,908] Trial 0 finished with value: 5.0 and parameters: {'y': 5}. Best is trial 0 with value: 5.0.\n",
      "[I 2023-10-21 16:04:09,910] Trial 1 finished with value: 2.0 and parameters: {'y': 2}. Best is trial 1 with value: 2.0.\n",
      "[I 2023-10-21 16:04:09,911] Trial 2 finished with value: 4.0 and parameters: {'y': 4}. Best is trial 1 with value: 2.0.\n",
      "[I 2023-10-21 16:04:09,913] Trial 3 finished with value: 6.0 and parameters: {'y': 6}. Best is trial 1 with value: 2.0.\n",
      "[I 2023-10-21 16:04:09,916] Trial 4 finished with value: 3.0 and parameters: {'y': 3}. Best is trial 1 with value: 2.0.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "def objective(trial):\n",
    "    y = trial.suggest_int(\"y\", -99, 99)\n",
    "    return y\n",
    "\n",
    "\n",
    "search_space = {\"y\": [2,3,4,5,6]}\n",
    "study = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space))\n",
    "study.optimize(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestRedeNeural.test_config_rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 7 - m√©todo `forward_propagation`**: Execute, para todas as camadas, o m√©todo forward_propagation. Note que, para primeira camada,\n",
    "    a entrada ser√° a matriz $X$, representada por `mat_x` e, as demais, seriam a matriz $A^{[l-1]}$, ou seja, a matriz de ativa√ß√µes da camada anterior. Lembre-se que cada camada possui sua matriz de ativa√ß√µes representada\n",
    "    pelo atributo `mat_a` e temos o vetor de camadas para podemos acessar as camadas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestRedeNeural.test_forward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 8 - m√©todo `backward_propagation`**: Execute, para todas as camadas, o m√©todo backward_propagation. Fique atento na ordem de execu√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestRedeNeural.test_backward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 9 - m√©todo `fit`**: SImilar ao m√©todo `fit` que implementamos na pr√°tica de Regress√£o Log√≠stica, realiza self.num_iteracoes itera√ß√µes (√©pocas) da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itera√ß√£o: 0 Loss: 0.694204119700994\n",
      "Itera√ß√£o: 100 Loss: 0.3906298846775872\n",
      "Itera√ß√£o: 200 Loss: 0.03754945414496613\n",
      "Itera√ß√£o: 300 Loss: 0.014358557992396569\n",
      "Itera√ß√£o: 400 Loss: 0.008209361698326761\n",
      "Itera√ß√£o: 500 Loss: 0.005564690230902035\n",
      "Itera√ß√£o: 600 Loss: 0.004137196957806488\n",
      "Itera√ß√£o: 700 Loss: 0.0032583489644620075\n",
      "Itera√ß√£o: 800 Loss: 0.0026688488652997066\n",
      "Itera√ß√£o: 900 Loss: 0.0022488829976152376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.032s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestRedeNeural.test_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 10 - m√©todo `loss_function`**: Para calcular o `loss_function`, voc√™ dever√° obter o vetor de ativa√ß√µes $\\pmb{a}$ (representado por `arr_a` em cada unidade) apropriado. Fique atento em qual camada/unidade voc√™ dever√° obter o arr_a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 11 - m√©todo `predict`**: similar ao da regress√£o logistica, para uma determinada matriz $X$ representando todos os atributos a serem preditos, calcula a predi√ß√£o $\\pmb{\\hat{y}}$ para cada inst√¢ncia $\\in X$ por meio do m√©todo `forward_propagation` de um modelo j√° criado. Esse c√≥digo ser√° muito similar ao que fizemos na pr√°tica de regress√£o logistica, por√©m, dever√° ficar atento a qual camada/unidade voc√™ dever√° obter o vetor de ativa√ß√µes arr_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itera√ß√£o: 0 Loss: 0.694204119700994\n",
      "Itera√ß√£o: 100 Loss: 0.3906298846775872\n",
      "Itera√ß√£o: 200 Loss: 0.03754945414496613\n",
      "Itera√ß√£o: 300 Loss: 0.014358557992396569\n",
      "Itera√ß√£o: 400 Loss: 0.008209361698326761\n",
      "Itera√ß√£o: 500 Loss: 0.005564690230902035\n",
      "Itera√ß√£o: 600 Loss: 0.004137196957806488\n",
      "Itera√ß√£o: 700 Loss: 0.0032583489644620075\n",
      "Itera√ß√£o: 800 Loss: 0.0026688488652997066\n",
      "Itera√ß√£o: 900 Loss: 0.0022488829976152376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projetos\\Python\\12 - Deep Learning\\AM-DeepLearning\\rede_neural_profunda_test.py:370: DeprecationWarning: Please use assertEqual instead.\n",
      "  self.assertEquals(len(arr_predicts),5, \"Quantidade de resultados inesperada\")\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.022s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m rede_neural_profunda_test TestRedeNeural.test_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo - Rede Neural Funcionando*  ü§© "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Assim espera-se ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usaremos o mesmo dataset da pr√°tica de regrss√£o logistica:\n",
    "mat_x, arr_y = sklearn.datasets.make_moons(400, noise=0.05)\n",
    "plt.scatter(mat_x[:,0], mat_x[:,1], s=40, c=arr_y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie uma rede neural de duas camadas a primeira com 4 unidades escondidas e, a segunda, com 1 unidade (essa seria a camada de sa√≠da). A fun√ß√£o de ativa√ß√£o da primeira camada ser√° uma Tangente Hiperb√≥lica (`tanh`) e, da segunda, uma `sigmoid`. Rode por 3.000 √©pocas. Ao trainar, coloque como learning rate=1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = None\n",
    "dnn.fit(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(dnn,mat_x,arr_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fa√ßa testes aumentando o n√∫mero de unidades, a fun√ß√£o de ativa√ß√£o e/ou aumentando a profundidade da rede neural. Descreva abaixo o que foi observado. Perceba que, nesta pr√°tica, fazer uma rede neural profunda geralmente n√£o ir√° melhorar o modelo. Um dos motivos √© que temos apenas 2 atributos. Uma rede neural profunda tende a melhorar o resultado quando temos diversos atributos, pois, em cada camada reduzimos a dimensionalidade do problema. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
